<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Class 1 有监督机器学习回归和分类Week3 逻辑回归二元分类这种只有两种可能输出的分类问题称为二元分类。 sigmoid 函数                输入特征，输出0到1 决策边界线性or非线性 代价函数平方误差成本函数不是逻辑回归的理想成本函数。     请记住，损失函数衡量的是你在一个训练样例上的表现如何，它是通过总结你随后获得的所有训练样例的损失，成本函数衡量你在整个训练集上">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="http://example.com/2023/10/08/MachineLearningAndrew/index.html">
<meta property="og:site_name" content="枫叶苑">
<meta property="og:description" content="Class 1 有监督机器学习回归和分类Week3 逻辑回归二元分类这种只有两种可能输出的分类问题称为二元分类。 sigmoid 函数                输入特征，输出0到1 决策边界线性or非线性 代价函数平方误差成本函数不是逻辑回归的理想成本函数。     请记住，损失函数衡量的是你在一个训练样例上的表现如何，它是通过总结你随后获得的所有训练样例的损失，成本函数衡量你在整个训练集上">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008173829773.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008173915794.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008174151181.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008174227410.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008205513401.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008205613800.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008205638121.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008205807667.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008210426343.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008211105952.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008211415275.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008211755913.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008211932119.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008211950319.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008223908932.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008224030737.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008224109310.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008225551317.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231009222124079.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231009222647437.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231009224920509.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231009224946845.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010125603129.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010125701208.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010130715345.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010130841602.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010204558837.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010204715345.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010212144511.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010212557352.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010220105018.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010220327241.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010221115720.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010221418604.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010221520301.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010221537117.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010231015869.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010231157191.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010231710233.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231010235541548.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011083027133.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011083802084.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011084012544.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011084442975.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011091646019.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011091757791.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011092256493.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011093159868.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011093401564.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011093433520.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011093859951.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011094124259.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011094256539.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011103735186.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011104512509.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011104852894.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011105426342.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011110222504.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011110708303.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011111707005.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011112028987.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011112250065.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011112605831.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011112929003.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011113910756.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011114442995.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011115040740.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011115116926.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011115211067.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011115324412.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011131113956.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011163140324.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011163230072.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011163351440.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011163931188.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011164414049.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011165618174.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011170439331.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231011170740787.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012075401986.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012075410387.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012081611286.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012082638836.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012082823450.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012083632353.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012083921344.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012084601880.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012085239987.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012090404205.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012091154958.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012092643280.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012093230276.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012094738491.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012095356821.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012095814528.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012125528206.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012125536447.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012130426099.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012145049473.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012150757199.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012151542672.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012151925053.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012152146445.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012152706177.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012152845235.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012153346742.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012154133115.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012154923207.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012215712447.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231012220215931.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013082659472.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013084827450.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013084924028.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013085546244.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013090207839.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013090413780.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013091417801.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013092525171.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013092709205.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013093541589.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013094256436.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013094536963.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013095022269.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013095116749.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013095506383.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013095650568.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013095901608.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013100113430.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013143727458.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013144130791.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013144725579.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013144803015.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013145539351.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013145951931.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013150021292.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013150639264.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013150936617.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013151110932.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013151407807.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013151905392.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013152534237.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013152823434.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013153036731.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013153343967.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013154612258.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013170727751.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013171217861.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013171447596.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013172253829.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013172530272.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013172923497.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013185527334.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013185513926.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013191147456.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013191515723.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013191936751.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013192130503.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013214059896.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013214515098.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013220256063.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013221245911.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013221652384.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013221916868.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013222607911.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013222846102.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013223922575.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013224639866.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013225256166.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013225733542.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013230506858.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013230853479.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013231149426.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013231255361.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013231653864.png">
<meta property="og:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231013232030585.png">
<meta property="article:published_time" content="2023-10-08T09:01:13.479Z">
<meta property="article:modified_time" content="2023-10-14T16:05:03.766Z">
<meta property="article:author" content="枫叶">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/10/08/MachineLearningAndrew/image-20231008173829773.png">

<link rel="canonical" href="http://example.com/2023/10/08/MachineLearningAndrew/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Machine Learning | 枫叶苑</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">枫叶苑</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/10/08/MachineLearningAndrew/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="枫叶">
      <meta itemprop="description" content="今天科研了吗">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="枫叶苑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-10-08 17:01:13" itemprop="dateCreated datePublished" datetime="2023-10-08T17:01:13+08:00">2023-10-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-10-15 00:05:03" itemprop="dateModified" datetime="2023-10-15T00:05:03+08:00">2023-10-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Class-1-有监督机器学习回归和分类"><a href="#Class-1-有监督机器学习回归和分类" class="headerlink" title="Class 1 有监督机器学习回归和分类"></a>Class 1 有监督机器学习回归和分类</h1><h2 id="Week3-逻辑回归"><a href="#Week3-逻辑回归" class="headerlink" title="Week3 逻辑回归"></a>Week3 逻辑回归</h2><h3 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h3><p>这种只有两种可能输出的分类问题称为二元分类。</p>
<h3 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231008173829773.png" alt="image-20231008173829773"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008173915794.png" alt="image-20231008173915794"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008174151181.png" alt="image-20231008174151181">             <img src="/2023/10/08/MachineLearningAndrew/image-20231008174227410.png" alt="image-20231008174227410"></p>
<p>输入特征，输出0到1</p>
<h3 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h3><p>线性or非线性</p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>平方误差成本函数不是逻辑回归的理想成本函数。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008205513401.png" alt="image-20231008205513401"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008205613800.png" alt="image-20231008205613800"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008205638121.png" alt="image-20231008205638121"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008205807667.png" alt="image-20231008205807667"></p>
<p>请记住，损失函数衡量的是你在一个训练样例上的表现如何，它是通过总结你随后获得的所有训练样例的损失，成本函数衡量你在整个训练集上的表现。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008210426343.png" alt="image-20231008210426343"></p>
<p>整体成本函数为凸函数，可以获得全局最小值</p>
<p>简化损失函数</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008211105952.png" alt="image-20231008211105952"></p>
<p>代价函数</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008211415275.png" alt="image-20231008211415275"></p>
<p>使用最大似然估计推导出来，是凸函数</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231008211755913.png" alt="image-20231008211755913"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008211932119.png" alt="image-20231008211932119"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008211950319.png" alt="image-20231008211950319"></p>
<p>Same concepts:</p>
<ul>
<li>Monitor gradient descent(learning curve)</li>
<li>Vectorized implementation</li>
<li>Feature scaling</li>
</ul>
<h3 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h3><p>欠拟合：高偏差 high bias  Does not fit thetraining set well</p>
<p>过拟合：高方差 high variance Fits the training setextremely well</p>
<h3 id="解决过拟合"><a href="#解决过拟合" class="headerlink" title="解决过拟合"></a>解决过拟合</h3><p>收集更多的训练样本</p>
<p>选择特征   select features to include&#x2F;exclude</p>
<p>正则化 Regularization 正则化是一种更温和地减少某些特征影响的方法，而不用像彻底消除它那样严厉。</p>
<p>那么正则化的作用是，它可以让你保留所有特征，但它们只是防止特征产生过大的影响，而这有时会导致过度拟合。</p>
<h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h3><p>λ正则化参数</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008223908932.png" alt="image-20231008223908932"></p>
<p>正则化线性回归</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008224030737.png" alt="image-20231008224030737"></p>
<p>梯度下降</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231008224109310.png" alt="image-20231008224109310"></p>
<p>正则化逻辑回归</p>
<h1 id><a href="#" class="headerlink" title></a><img src="/2023/10/08/MachineLearningAndrew/image-20231008225551317.png" alt="image-20231008225551317"></h1><h1 id="Class-3-深度学习"><a href="#Class-3-深度学习" class="headerlink" title="Class 3 深度学习"></a>Class 3 深度学习</h1><h2 id="Week1-神经网络"><a href="#Week1-神经网络" class="headerlink" title="Week1 神经网络"></a>Week1 神经网络</h2><p>输入层 输入为特征向量</p>
<p>隐藏层</p>
<p>输出层</p>
<p><strong>Tensorflow and Keras</strong><br>Tensorflow is a machine learning package developed by Google. In 2019, Google integrated Keras into Tensorflow and released Tensorflow 2.0. Keras is a framework developed independently by François Chollet that creates a simple, layer-centric interface to Tensorflow. This course will be using the Keras interface. </p>
<p>激活函数g(x)是sigmoid函数</p>
<p>前向传播算法</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231009222124079.png" alt="image-20231009222124079"></p>
<p>从左到右向前计算，称为前向传播</p>
<p>离输出层越近，隐藏层神经元越少</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231009222647437.png" alt="image-20231009222647437"></p>
<p>两个特征，第一个隐藏层有三个神经元，激活函数为sigmoid，输出为a1，第二层同理</p>
<p>输入向量要写成二维矩阵形式 x &#x3D; np.array([[200,17]])  1x2矩阵</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231009224920509.png" alt="image-20231009224920509"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231009224946845.png" alt="image-20231009224946845"></p>
<p>AGI 通用人工智能</p>
<h2 id="Week2-Tenserflow实现"><a href="#Week2-Tenserflow实现" class="headerlink" title="Week2 Tenserflow实现"></a>Week2 Tenserflow实现</h2><p><img src="/2023/10/08/MachineLearningAndrew/image-20231010125603129.png" alt="image-20231010125603129"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010125701208.png" alt="image-20231010125701208"></p>
<p>交叉熵损失函数</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010130715345.png" alt="image-20231010130715345"></p>
<p>回归和分类使用不同的损失代价函数</p>
<p>梯度下降</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010130841602.png" alt="image-20231010130841602"></p>
<p>多层感知器：多层神经网路</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="ReLU-Activation"><a href="#ReLU-Activation" class="headerlink" title="ReLU Activation"></a>ReLU Activation</h4><p>This week, a new activation was introduced, the Rectified Linear Unit (ReLU).</p>
<p>𝑎&#x3D;𝑚𝑎𝑥(0,𝑧)# ReLU function<img src="/2023/10/08/MachineLearningAndrew/image-20231010204558837.png" alt="image-20231010204558837"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010204715345.png" alt="image-20231010204715345"></p>
<p>二分类问题，sigmoid激活函数是最自然的选择，输出层使用</p>
<p>回归模型，输出有正有负，预测明天的股票价格，输出层使用线性激活函数</p>
<p>回归模型，输出为非负，预测房屋价格，非负，输出层选择ReLU函数</p>
<p>隐藏层选择Relu函数，ReLU计算速度更快，效率高，但事实证明更重要的第二个原因是ReLU函数仅在图形的一部分变平;左边这里<br>是完全平坦的，而sigmoid激活函数，它在两个地方变得平坦。梯度下降就会很慢，减慢学习速度</p>
<p>为什么要使用激活函数？</p>
<p>若所有层都是用线性激活函数，那就变成了线性回归</p>
<h3 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h3><h3 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231010212144511.png" alt="image-20231010212144511"></p>
<h4 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231010212557352.png" alt="image-20231010212557352"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010220105018.png" alt="image-20231010220105018"></p>
<h4 id="Tenserflow-实现"><a href="#Tenserflow-实现" class="headerlink" title="Tenserflow 实现"></a>Tenserflow 实现</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231010220327241.png" alt="image-20231010220327241"></p>
<h4 id="改进实现"><a href="#改进实现" class="headerlink" title="改进实现"></a>改进实现</h4><p>避免计算过程中出现过大或者过小值造成计算错误，改进方法在计算过程中进行了重新排列</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010221115720.png" alt="image-20231010221115720"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010221418604.png" alt="image-20231010221418604"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010221520301.png" alt="image-20231010221520301"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010221537117.png" alt="image-20231010221537117"></p>
<h3 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h3><p>一个神经网络同时检测多个目标</p>
<h3 id="更快的训练方法"><a href="#更快的训练方法" class="headerlink" title="更快的训练方法"></a>更快的训练方法</h3><h4 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h4><p>自动调节α</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010231015869.png" alt="image-20231010231015869"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010231157191.png" alt="image-20231010231157191"></p>
<h3 id="其他的网络层"><a href="#其他的网络层" class="headerlink" title="其他的网络层"></a>其他的网络层</h3><p>密集层</p>
<p>卷积层</p>
<ul>
<li><p>更快的计算</p>
</li>
<li><p>需要更少的训练数据，不太会过拟合</p>
</li>
</ul>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231010231710233.png" alt="image-20231010231710233"></p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231010235541548.png" alt="image-20231010235541548"></p>
<h2 id="Week3-在机器学习项目中下一步该做什么"><a href="#Week3-在机器学习项目中下一步该做什么" class="headerlink" title="Week3 在机器学习项目中下一步该做什么"></a>Week3 在机器学习项目中下一步该做什么</h2><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011083027133.png" alt="image-20231011083027133"></p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>训练集分为两个子集</p>
<p>training set</p>
<p>test set</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011083802084.png" alt="image-20231011083802084"></p>
<p>不包含正则化项</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011084012544.png" alt="image-20231011084012544"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011084442975.png" alt="image-20231011084442975"></p>
<p>就是相比于之前计算在测试集和训练集上误差的那两个公式，我们更常用模型分类错误的次数除以总的预测次数来表示误差</p>
<p>就是对于逻辑回归来说，可以通过计算误判占比的方法来代表成本函数，比如test set中误判的占比是10％，那么J test就是0.1</p>
<h4 id="分为三个子集，训练集、交叉验证集、测试集"><a href="#分为三个子集，训练集、交叉验证集、测试集" class="headerlink" title="分为三个子集，训练集、交叉验证集、测试集"></a>分为三个子集，训练集、交叉验证集、测试集</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011091646019.png" alt="image-20231011091646019"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011091757791.png" alt="image-20231011091757791"></p>
<p>选择最小的交叉验证集误差对应的模型</p>
<p>用测试集来评估泛化误差</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011092256493.png" alt="image-20231011092256493"></p>
<h4 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011093159868.png" alt="image-20231011093159868"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011093401564.png" alt="image-20231011093401564"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011093433520.png" alt="image-20231011093433520"></p>
<p>正则化如何影响偏差和方差，从而影响算法的性能</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011093859951.png" alt="image-20231011093859951"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011094124259.png" alt="image-20231011094124259"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011094256539.png" alt="image-20231011094256539"></p>
<h4 id="指定一个用于性能评估的基准"><a href="#指定一个用于性能评估的基准" class="headerlink" title="指定一个用于性能评估的基准"></a>指定一个用于性能评估的基准</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011103735186.png" alt="image-20231011103735186"></p>
<p>竞争算法</p>
<h4 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011104512509.png" alt="image-20231011104512509"></p>
<p>训练集变大，误差增大</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011104852894.png" alt="image-20231011104852894"></p>
<p>这给出了这个结论，也许有点令人惊讶，如果学习算法具有高偏差，获得更多的训<br>练数据本身就没有那么大的希望。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011105426342.png" alt="image-20231011105426342"></p>
<p>所以在这种情况下，可能仅仅通过增加训练集的大小来降低交叉验证误差并让你的算法表现得越来越好，这与高偏差情况不同，在这种情况下你唯一要做的就是获得更多的训练数据，实际上并不能帮助您了解算法性能。</p>
<h4 id="如何改进"><a href="#如何改进" class="headerlink" title="如何改进"></a>如何改进</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011110222504.png" alt="image-20231011110222504"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011110708303.png" alt="image-20231011110708303"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011111707005.png" alt="image-20231011111707005"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011112028987.png" alt="image-20231011112028987"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011112250065.png" alt="image-20231011112250065"></p>
<h3 id="机器学习开发的迭代"><a href="#机器学习开发的迭代" class="headerlink" title="机器学习开发的迭代"></a>机器学习开发的迭代</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011112605831.png" alt="image-20231011112605831"></p>
<p>垃圾邮件分类</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011112929003.png" alt="image-20231011112929003"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011113910756.png" alt="image-20231011113910756"></p>
<h4 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011114442995.png" alt="image-20231011114442995"></p>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>旋转图像扭曲放大缩小</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011115040740.png" alt="image-20231011115040740"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011115116926.png" alt="image-20231011115116926"></p>
<p>音频增强 </p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011115211067.png" alt="image-20231011115211067"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011115324412.png" alt="image-20231011115324412"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011131113956.png" alt="image-20231011131113956"></p>
<h4 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 Transfer  learning"></a>迁移学习 Transfer  learning</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011163140324.png" alt="image-20231011163140324"></p>
<p>数据集小选择1，数据集大选择二</p>
<p>先在大的数据集训练（监督与训练），再在小的训练称为微调</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011163230072.png" alt="image-20231011163230072"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011163351440.png" alt="image-20231011163351440"></p>
<h4 id="机器学习项目全周期"><a href="#机器学习项目全周期" class="headerlink" title="机器学习项目全周期"></a>机器学习项目全周期</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231011163931188.png" alt="image-20231011163931188"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011164414049.png" alt="image-20231011164414049"></p>
<h4 id="倾斜数据集的误差指标"><a href="#倾斜数据集的误差指标" class="headerlink" title="倾斜数据集的误差指标"></a>倾斜数据集的误差指标</h4><p>精确度和召回率</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011165618174.png" alt="image-20231011165618174"></p>
<p>精度和召回率</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011170439331.png" alt="image-20231011170439331"></p>
<p>F1score 更强调P和R中较低的那个 调和平均值</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231011170740787.png" alt="image-20231011170740787"></p>
<h2 id="Week4-决策树"><a href="#Week4-决策树" class="headerlink" title="Week4 决策树"></a>Week4 决策树</h2><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012075401986.png" alt="image-20231012075401986"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012075410387.png" alt="image-20231012075410387"></p>
<p>根节点 决策节点 叶节点</p>
<p>决策树学习算法的工作是，从所有可能的决策树中，尝试选择一个希望在训练集上<br>表现良好的树，然后理想地泛化到新数据，例如交叉验证和测试集</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><p>决策树学习的第一步是，我们必须决定在根节点使用什么特征。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012081611286.png" alt="image-20231012081611286"></p>
<p>Decision 2: When do you stop splitting?</p>
<ul>
<li>When a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum depth</li>
<li>When improvements in purity score are below a threshold</li>
<li>When number of examples in a node is below a threshold</li>
</ul>
<p>您可能想要限制决策树深度的一个原因是确保我们的树不会变得太大和笨重，其<br>次，通过保持树小，它不太容易过度拟合。</p>
<h3 id="纯度"><a href="#纯度" class="headerlink" title="纯度"></a>纯度</h3><h4 id="熵-entropy"><a href="#熵-entropy" class="headerlink" title="熵 entropy"></a>熵 entropy</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012082638836.png" alt="image-20231012082638836"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012082823450.png" alt="image-20231012082823450"></p>
<p>选择拆分信息增益</p>
<p>减少熵</p>
<p>信息增益 分之前的熵减去分后熵的加权平均</p>
<p>停止标准，每次信息增益如果太小停止分类</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012083632353.png" alt="image-20231012083632353"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012083921344.png" alt="image-20231012083921344"></p>
<h4 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h4><ul>
<li>Start with all examples at the root node</li>
<li>Calculate information gain for all possible features, and pick the one with the highest information gain</li>
<li>Split dataset according to selected feature,and create left and right branches of the tree</li>
<li>Keep repeating splitting process until stopping criteria is met:<ul>
<li>when a node is 100% one class</li>
<li>When splitting a node will result in the tree exceeding a maximum depth</li>
<li>Information gain from additional splits is less than threshold</li>
<li>When number of examples in a node is below a threshold</li>
</ul>
</li>
</ul>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012084601880.png" alt="image-20231012084601880"></p>
<p>递归分类</p>
<h3 id="独热编码-one-hot"><a href="#独热编码-one-hot" class="headerlink" title="独热编码 one-hot"></a>独热编码 one-hot</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012085239987.png" alt="image-20231012085239987"></p>
<p>通过one-hot编码，您可以让决策树处理可以采用两个以上离散值的特征，您还可以将其应用于新网络或线性回归或逻辑回归训练。</p>
<p>连续值</p>
<p>尝试不同的阈值，计算纯度</p>
<h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012090404205.png" alt="image-20231012090404205"></p>
<p>尝试减少方差</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012091154958.png" alt="image-20231012091154958"></p>
<h3 id="使用多个决策树"><a href="#使用多个决策树" class="headerlink" title="使用多个决策树"></a>使用多个决策树</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012092643280.png" alt="image-20231012092643280"></p>
<h4 id="有放回的采样"><a href="#有放回的采样" class="headerlink" title="有放回的采样"></a>有放回的采样</h4><p>构建新的数据集，</p>
<p>Random Forest Algorithm</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012093230276.png" alt="image-20231012093230276"></p>
<p>Randomizing the feature choice<br>At each node, when choosing a feature to use to split,if n features are available, pick a random subset of k &lt;n features and allow the algorithm to only choose from that subset of features.</p>
<p>k的选择（根号n，log2(n)）</p>
<h4 id="XGBoost-extreme-Gradient-Boosting"><a href="#XGBoost-extreme-Gradient-Boosting" class="headerlink" title="XGBoost (extreme Gradient Boosting)"></a>XGBoost (extreme Gradient Boosting)</h4><p>内置正则化防止过度拟合</p>
<ul>
<li>Open source implementation of boosted trees</li>
<li>Fast efficient implementation</li>
<li>Good choice of default splitting criteria and criteria for when to stop splitting</li>
<li>Built in regularization to prevent over fitting</li>
<li>Highly competitive algorithm for machine learning competitions (eg: Kaggle competitions)</li>
</ul>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012094738491.png" alt="image-20231012094738491"></p>
<h3 id="何时使用决策树"><a href="#何时使用决策树" class="headerlink" title="何时使用决策树"></a>何时使用决策树</h3><p>表格数据</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012095356821.png" alt="image-20231012095356821"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012095814528.png" alt="image-20231012095814528"></p>
<h1 id="Class3-无监督学习"><a href="#Class3-无监督学习" class="headerlink" title="Class3 无监督学习"></a>Class3 无监督学习</h1><h2 id="Week1-无监督学习"><a href="#Week1-无监督学习" class="headerlink" title="Week1 无监督学习"></a>Week1 无监督学习</h2><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><h4 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012125528206.png" alt="image-20231012125528206"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012125536447.png" alt="image-20231012125536447"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012130426099.png" alt="image-20231012130426099"></p>
<p>如果一个集群训练样本为零，可以消除该集群，最终得到k-1；另一种方法是重新初始化该集群质心</p>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012145049473.png" alt="image-20231012145049473"></p>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012150757199.png" alt="image-20231012150757199"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012151542672.png" alt="image-20231012151542672"></p>
<h4 id="选择聚类数量"><a href="#选择聚类数量" class="headerlink" title="选择聚类数量"></a>选择聚类数量</h4><p>肘法</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012151925053.png" alt="image-20231012151925053"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012152146445.png" alt="image-20231012152146445"></p>
<h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012152706177.png" alt="image-20231012152706177"></p>
<h4 id="密度估计"><a href="#密度估计" class="headerlink" title="密度估计"></a>密度估计</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012152845235.png" alt="image-20231012152845235"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012153346742.png" alt="image-20231012153346742"></p>
<h4 id="高斯正态分布"><a href="#高斯正态分布" class="headerlink" title="高斯正态分布"></a>高斯正态分布</h4><p>最大似然估计</p>
<h4 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012154133115.png" alt="image-20231012154133115"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012154923207.png" alt="image-20231012154923207"></p>
<h4 id="开发与评估异常检测系统"><a href="#开发与评估异常检测系统" class="headerlink" title="开发与评估异常检测系统."></a>开发与评估异常检测系统.</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231012215712447.png" alt="image-20231012215712447"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231012220215931.png" alt="image-20231012220215931"></p>
<p>这种替代方案的缺点是，在调整算法后，您没有公平的方法来判断它在未来示例中的实际效果如何，因为您没有测试集。</p>
<p>当你的数据集很小的时候，特别是当你有异常的数量时，你的数据集很小，这可能是你最好的选择。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013082659472.png" alt="image-20231013082659472"></p>
<h4 id="异常检测vs监督学习"><a href="#异常检测vs监督学习" class="headerlink" title="异常检测vs监督学习"></a>异常检测vs监督学习</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013084827450.png" alt="image-20231013084827450"></p>
<p>例子</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013084924028.png" alt="image-20231013084924028"></p>
<h4 id="选择用什么特征"><a href="#选择用什么特征" class="headerlink" title="选择用什么特征"></a>选择用什么特征</h4><p>特征改变为高斯分布</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013085546244.png" alt="image-20231013085546244"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013090207839.png" alt="image-20231013090207839"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013090413780.png" alt="image-20231013090413780"></p>
<h2 id="Week2-推荐系统"><a href="#Week2-推荐系统" class="headerlink" title="Week2 推荐系统"></a>Week2 推荐系统</h2><h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><h4 id="使用每个特征数据"><a href="#使用每个特征数据" class="headerlink" title="使用每个特征数据"></a>使用每个特征数据</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013091417801.png" alt="image-20231013091417801"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013092525171.png" alt="image-20231013092525171"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013092709205.png" alt="image-20231013092709205"></p>
<h4 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h4><p>假设已经有了w和b，猜测特征x</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013093541589.png" alt="image-20231013093541589"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013094256436.png" alt="image-20231013094256436"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013094536963.png" alt="image-20231013094536963"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013095022269.png" alt="image-20231013095022269"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013095116749.png" alt="image-20231013095116749"></p>
<p>这种协同过滤是从多个用户收集数据，用户之间的这种协作可帮助您预测未来甚至其他用户的评级。</p>
<p>推荐系统的一个非常常见的用例是当您有二进制标签时，例如用户喜欢、喜欢或与项目交互的标签。</p>
<h4 id="二进制标签"><a href="#二进制标签" class="headerlink" title="二进制标签"></a>二进制标签</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013095506383.png" alt="image-20231013095506383"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013095650568.png" alt="image-20231013095650568"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013095901608.png" alt="image-20231013095901608"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013100113430.png" alt="image-20231013100113430"></p>
<p>分类不用正则化</p>
<h4 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013143727458.png" alt="image-20231013143727458"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013144130791.png" alt="image-20231013144130791"></p>
<p>对未知用户，预测评分为均值</p>
<h4 id="协同过滤Tensorflow实现"><a href="#协同过滤Tensorflow实现" class="headerlink" title="协同过滤Tensorflow实现"></a>协同过滤Tensorflow实现</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013144725579.png" alt="image-20231013144725579"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013144803015.png" alt="image-20231013144803015"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013145539351.png" alt="image-20231013145539351"></p>
<h4 id="寻找相关特征"><a href="#寻找相关特征" class="headerlink" title="寻找相关特征"></a>寻找相关特征</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013145951931.png" alt="image-20231013145951931"></p>
<p>协同过滤的局限性</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013150021292.png" alt="image-20231013150021292"></p>
<p>冷启动问题</p>
<p>边缘信息</p>
<h4 id="基于内容的过滤算法"><a href="#基于内容的过滤算法" class="headerlink" title="基于内容的过滤算法"></a>基于内容的过滤算法</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013150639264.png" alt="image-20231013150639264"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013150936617.png" alt="image-20231013150936617"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013151110932.png" alt="image-20231013151110932"></p>
<p>如何计算V</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013151407807.png" alt="image-20231013151407807"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013151905392.png" alt="image-20231013151905392"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013152534237.png" alt="image-20231013152534237"></p>
<h4 id="从大型目录中推荐"><a href="#从大型目录中推荐" class="headerlink" title="从大型目录中推荐"></a>从大型目录中推荐</h4><p>两个步骤：检索和排名</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013152823434.png" alt="image-20231013152823434"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013153036731.png" alt="image-20231013153036731"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013153343967.png" alt="image-20231013153343967"></p>
<h4 id="基于内容的Tensorflow实现"><a href="#基于内容的Tensorflow实现" class="headerlink" title="基于内容的Tensorflow实现"></a>基于内容的Tensorflow实现</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013154612258.png" alt="image-20231013154612258"></p>
<h4 id="降低特征数量"><a href="#降低特征数量" class="headerlink" title="降低特征数量"></a>降低特征数量</h4><p>PCA主成分分析法，特征降为二维或者三维，便于可视化</p>
<h4 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013170727751.png" alt="image-20231013170727751"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013171217861.png" alt="image-20231013171217861"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013171447596.png" alt="image-20231013171447596"></p>
<p>当使用线性回归来预测目标输出Y并且PCA试图获取大量特征并平等对待它们并减少很好地表示数据所需的轴数</p>
<p>因此，如果您尝试预测y的值，则应使用线性回归;如果您尝试减少数据集中的特征数量，例如将其可视化，则应使用PCA。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013172253829.png" alt="image-20231013172253829"></p>
<p>每个特征的方差贡献率</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013172530272.png" alt="image-20231013172530272"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013172923497.png" alt="image-20231013172923497"></p>
<h2 id="Week3-强化学习"><a href="#Week3-强化学习" class="headerlink" title="Week3 强化学习"></a>Week3 强化学习</h2><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013185527334.png" alt="image-20231013185527334"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013185513926.png" alt="image-20231013185513926"></p>
<p>状态、动作、奖励和下一个状态（s，a，R（s），s‘)</p>
<h4 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h4><p>折扣因子</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013191147456.png" alt="image-20231013191147456"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013191515723.png" alt="image-20231013191515723"></p>
<h4 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h4><p>pi</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013191936751.png" alt="image-20231013191936751"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013192130503.png" alt="image-20231013192130503"></p>
<p>马尔可夫决策过程</p>
<p>在马尔可夫决策过程中，未来只取决于你现在所处的位置，而不取决于你是如何到达这里的。</p>
<h4 id="状态-动作价值函数"><a href="#状态-动作价值函数" class="headerlink" title="状态-动作价值函数"></a>状态-动作价值函数</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013214059896.png" alt="image-20231013214059896"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013214515098.png" alt="image-20231013214515098"></p>
<h4 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013220256063.png" alt="image-20231013220256063"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013221245911.png" alt="image-20231013221245911"></p>
<p>如果你从状态s 开始，你将采取行动a，然后在此之后采取最佳行动，那么你将随着时间的推移看到一些奖励序列。</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013221652384.png" alt="image-20231013221652384"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013221916868.png" alt="image-20231013221916868"></p>
<h4 id="随机强化学习"><a href="#随机强化学习" class="headerlink" title="随机强化学习"></a>随机强化学习</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013222607911.png" alt="image-20231013222607911"></p>
<h4 id="连续状态"><a href="#连续状态" class="headerlink" title="连续状态"></a>连续状态</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013222846102.png" alt="image-20231013222846102"></p>
<p>连续马尔可夫 MTP</p>
<h4 id="学习状态值函数"><a href="#学习状态值函数" class="headerlink" title="学习状态值函数"></a>学习状态值函数</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013223922575.png" alt="image-20231013223922575"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013224639866.png" alt="image-20231013224639866"></p>
<p>不知道Q，随机猜测</p>
<h4 id="DQN-p146"><a href="#DQN-p146" class="headerlink" title="DQN  p146"></a>DQN  p146</h4><p>意思是神经网络里的参数随机初始化，然后(s’,a’)输入，得到maxQ的预测</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013225256166.png" alt="image-20231013225256166"></p>
<p>交给神经网络训练的参数y中一部分是随机初始化神经网络生成的，但还有一部分是包含了当前状态的信息的，所以当训练次数增多后，外部的输入信息会逐步冲刷掉初始化的随机信息，给出真正的Q函数估计</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013225733542.png" alt="image-20231013225733542"></p>
<h4 id="贪婪算法"><a href="#贪婪算法" class="headerlink" title="贪婪算法"></a>贪婪算法</h4><p>使用高ε开始，逐步降低直到0.01</p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013230506858.png" alt="image-20231013230506858"></p>
<h4 id="小批量和软更新"><a href="#小批量和软更新" class="headerlink" title="小批量和软更新"></a>小批量和软更新</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013230853479.png" alt="image-20231013230853479"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013231149426.png" alt="image-20231013231149426"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013231255361.png" alt="image-20231013231255361"></p>
<p><img src="/2023/10/08/MachineLearningAndrew/image-20231013231653864.png" alt="image-20231013231653864"></p>
<h4 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h4><p><img src="/2023/10/08/MachineLearningAndrew/image-20231013232030585.png" alt="image-20231013232030585"></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/10/08/DataStruction/" rel="prev" title="Data Struction">
      <i class="fa fa-chevron-left"></i> Data Struction
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Class-1-%E6%9C%89%E7%9B%91%E7%9D%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">Class 1 有监督机器学习回归和分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">Week3 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">1.1.1.</span> <span class="nav-text">二元分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid-%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">sigmoid 函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="nav-number">1.1.3.</span> <span class="nav-text">决策边界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.1.5.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">1.1.6.</span> <span class="nav-text">过拟合与欠拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">1.1.7.</span> <span class="nav-text">解决过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-Regularization"><span class="nav-number">1.1.8.</span> <span class="nav-text">正则化 Regularization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Class-3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">Class 3 深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.1.</span> <span class="nav-text">Week1 神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week2-Tenserflow%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.2.</span> <span class="nav-text">Week2 Tenserflow实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.1.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU-Activation"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">ReLU Activation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">3.2.2.</span> <span class="nav-text">多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.3.</span> <span class="nav-text">Softmax函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-1"><span class="nav-number">3.2.3.1.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tenserflow-%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.2.3.2.</span> <span class="nav-text">Tenserflow 实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.2.3.3.</span> <span class="nav-text">改进实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB"><span class="nav-number">3.2.4.</span> <span class="nav-text">多标签分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9B%B4%E5%BF%AB%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">3.2.5.</span> <span class="nav-text">更快的训练方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam%E7%AE%97%E6%B3%95"><span class="nav-number">3.2.5.1.</span> <span class="nav-text">Adam算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E7%9A%84%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="nav-number">3.2.6.</span> <span class="nav-text">其他的网络层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">3.2.7.</span> <span class="nav-text">计算图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week3-%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%8B%E4%B8%80%E6%AD%A5%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="nav-number">3.3.</span> <span class="nav-text">Week3 在机器学习项目中下一步该做什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">3.3.1.</span> <span class="nav-text">模型评估</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E4%B8%BA%E4%B8%89%E4%B8%AA%E5%AD%90%E9%9B%86%EF%BC%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">分为三个子集，训练集、交叉验证集、测试集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">偏差和方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A%E4%B8%80%E4%B8%AA%E7%94%A8%E4%BA%8E%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E7%9A%84%E5%9F%BA%E5%87%86"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">指定一个用于性能评估的基准</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">学习曲线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%94%B9%E8%BF%9B"><span class="nav-number">3.3.1.5.</span> <span class="nav-text">如何改进</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%9A%84%E8%BF%AD%E4%BB%A3"><span class="nav-number">3.3.2.</span> <span class="nav-text">机器学习开发的迭代</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">误差分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">数据增强</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-Transfer-learning"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">迁移学习 Transfer  learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E5%85%A8%E5%91%A8%E6%9C%9F"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">机器学习项目全周期</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%80%BE%E6%96%9C%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%8C%87%E6%A0%87"><span class="nav-number">3.3.2.5.</span> <span class="nav-text">倾斜数据集的误差指标</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week4-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">3.4.</span> <span class="nav-text">Week4 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-number">3.4.1.</span> <span class="nav-text">学习过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%AF%E5%BA%A6"><span class="nav-number">3.4.2.</span> <span class="nav-text">纯度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%86%B5-entropy"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">熵 entropy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E5%90%88"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">整合</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81-one-hot"><span class="nav-number">3.4.3.</span> <span class="nav-text">独热编码 one-hot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A0%91"><span class="nav-number">3.4.4.</span> <span class="nav-text">回归树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">3.4.5.</span> <span class="nav-text">使用多个决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E6%94%BE%E5%9B%9E%E7%9A%84%E9%87%87%E6%A0%B7"><span class="nav-number">3.4.5.1.</span> <span class="nav-text">有放回的采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XGBoost-extreme-Gradient-Boosting"><span class="nav-number">3.4.5.2.</span> <span class="nav-text">XGBoost (extreme Gradient Boosting)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%95%E6%97%B6%E4%BD%BF%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">3.4.6.</span> <span class="nav-text">何时使用决策树</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Class3-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">Class3 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.1.</span> <span class="nav-text">Week1 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">4.1.1.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#k-means"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">k-means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">优化目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">4.1.1.3.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E8%81%9A%E7%B1%BB%E6%95%B0%E9%87%8F"><span class="nav-number">4.1.1.4.</span> <span class="nav-text">选择聚类数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="nav-number">4.1.2.</span> <span class="nav-text">异常检测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">密度估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">高斯正态分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">异常检测算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E4%B8%8E%E8%AF%84%E4%BC%B0%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F"><span class="nav-number">4.1.2.4.</span> <span class="nav-text">开发与评估异常检测系统.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8Bvs%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.1.2.5.</span> <span class="nav-text">异常检测vs监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E7%94%A8%E4%BB%80%E4%B9%88%E7%89%B9%E5%BE%81"><span class="nav-number">4.1.2.6.</span> <span class="nav-text">选择用什么特征</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week2-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">4.2.</span> <span class="nav-text">Week2 推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">4.2.1.</span> <span class="nav-text">推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%AF%8F%E4%B8%AA%E7%89%B9%E5%BE%81%E6%95%B0%E6%8D%AE"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">使用每个特征数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">协同过滤算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%A0%87%E7%AD%BE"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">二进制标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">4.2.1.4.</span> <span class="nav-text">均值归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4Tensorflow%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.2.1.5.</span> <span class="nav-text">协同过滤Tensorflow实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%BB%E6%89%BE%E7%9B%B8%E5%85%B3%E7%89%B9%E5%BE%81"><span class="nav-number">4.2.1.6.</span> <span class="nav-text">寻找相关特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="nav-number">4.2.1.7.</span> <span class="nav-text">基于内容的过滤算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E5%A4%A7%E5%9E%8B%E7%9B%AE%E5%BD%95%E4%B8%AD%E6%8E%A8%E8%8D%90"><span class="nav-number">4.2.1.8.</span> <span class="nav-text">从大型目录中推荐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84Tensorflow%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.2.1.9.</span> <span class="nav-text">基于内容的Tensorflow实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%99%8D%E4%BD%8E%E7%89%B9%E5%BE%81%E6%95%B0%E9%87%8F"><span class="nav-number">4.2.1.10.</span> <span class="nav-text">降低特征数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA%E7%AE%97%E6%B3%95"><span class="nav-number">4.2.1.11.</span> <span class="nav-text">PCA算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week3-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.3.</span> <span class="nav-text">Week3 强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.3.1.</span> <span class="nav-text">强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5"><span class="nav-number">4.3.1.1.</span> <span class="nav-text">回报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96"><span class="nav-number">4.3.1.2.</span> <span class="nav-text">决策</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81-%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.1.3.</span> <span class="nav-text">状态-动作价值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="nav-number">4.3.1.4.</span> <span class="nav-text">贝尔曼方程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.3.1.5.</span> <span class="nav-text">随机强化学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E7%8A%B6%E6%80%81"><span class="nav-number">4.3.1.6.</span> <span class="nav-text">连续状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8A%B6%E6%80%81%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">4.3.1.7.</span> <span class="nav-text">学习状态值函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DQN-p146"><span class="nav-number">4.3.1.8.</span> <span class="nav-text">DQN  p146</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95"><span class="nav-number">4.3.1.9.</span> <span class="nav-text">贪婪算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E5%92%8C%E8%BD%AF%E6%9B%B4%E6%96%B0"><span class="nav-number">4.3.1.10.</span> <span class="nav-text">小批量和软更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-number">4.3.1.11.</span> <span class="nav-text">局限性</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="枫叶"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">枫叶</p>
  <div class="site-description" itemprop="description">今天科研了吗</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-10 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">枫叶</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  















  

  

</body>
</html>
